{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import image_embedding\n",
    "import image_processing\n",
    "import inputs as input_ops\n",
    "\n",
    "import numpy as np\n",
    "import utils_project \n",
    "#import utils_caltech\n",
    "import configuration\n",
    "import cifar10\n",
    "from cifar10 import img_size, num_channels, num_classes,_num_images_train\n",
    "import cv2\n",
    "utils = utils_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(image_train,train_batch_size):\n",
    "    # Number of images in the training-set.\n",
    "    num_images = len(images_train)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = images_train[idx, :, :, :]\n",
    "    x_batch = utils.preturb_images(x_batch)\n",
    "    y_batch = labels_train[idx, :]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_cls(images, labels, cls_true):\n",
    "    # Number of images.\n",
    "    num_images = len(images)\n",
    "    \n",
    "    #change size\n",
    "    #images = utils.preturb_images(images)\n",
    "    \n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_images:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_images)\n",
    "        x_batch = utils.preturb_images(images[i:j, :])\n",
    "        # Create a feed-dict with the images and labels\n",
    "        # between index i and j.\n",
    "        feed_dict = {X: x_batch,\n",
    "                     y_true: labels[i:j, :]}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = sess.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    return correct, cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"convol_dnn/leak_relu_1:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "height = 299\n",
    "width = 299\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, height, width, num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "with tf.name_scope(\"convol_dnn\"):\n",
    "    \n",
    "    #shape formula W2=(W1−F+2P)/S+1 \n",
    "    num_input_channels=3\n",
    "    num_inputs = (299*299*3)\n",
    "    num_filters = 64\n",
    "    kernel_size = 5\n",
    "    layern1 = utils.conv_layer(X,\"layer1\",\n",
    "                                       use_pooling=True,\n",
    "                                       dropout = False,\n",
    "                                       activation=\"leak_relu\",\n",
    "                                       num_inputs=num_inputs,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       kernel_size=kernel_size,\n",
    "                                       num_filters=num_filters,\n",
    "                                       padding = 'SAME')\n",
    "    \n",
    "    #num_inputs = 100*100*64\n",
    "    #pool formula (W1−F)/S+1  \n",
    "    #pool shape from utils_project.conv_layer ksize=[1, 2, 2, 1],strides=[1, 3, 3, 1]\n",
    "    layern2 = utils_project.conv_layer(layern1,\"layer2\",use_pooling=True,\n",
    "                                       dropout=True,\n",
    "                                       activation=\"leak_relu\",\n",
    "                                       num_inputs=100*100*64,\n",
    "                                       num_input_channels=64,\n",
    "                                       kernel_size=5,\n",
    "                                       num_filters=128,\n",
    "                                       padding = 'SAME',stride_pooling=[1,2,2,1])\n",
    "    #output shape [none, 50,50,128]\n",
    "    \n",
    "    \n",
    "    layern3 = utils_project.conv_layer(layern2,\"layer3\",use_pooling=True,\n",
    "                                       dropout=True,\n",
    "                                       activation=\"leak_relu\",\n",
    "                                       num_inputs=50*50*128,\n",
    "                                       num_input_channels=128,\n",
    "                                       kernel_size=5,\n",
    "                                       num_filters=200,\n",
    "                                       padding = 'SAME',\n",
    "                                       stride_pooling=[1,2,2,1])\n",
    "    #output shape [none, 24,24,200]\n",
    "    \n",
    "    #[None,50*50*300]\n",
    "    flatten_tensor,num_features = utils_project.flatten_layer(layern3)\n",
    "    \n",
    "    layer_fc1 = utils_project.new_fc_layer(input=flatten_tensor,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=200,\n",
    "                         activation=\"leak_relu\")\n",
    "    \n",
    "    layer_fc2 = utils_project.new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=200,\n",
    "                         num_outputs=10,\n",
    "                         activation=\"leak_relu\")\n",
    "\n",
    "    \n",
    "    y_pred = tf.nn.softmax(layer_fc2)\n",
    "    \n",
    "    y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    print(layer_fc2)\n",
    "    #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(layer_fc2, y_true)\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(layer_fc2, y_true)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/cifar-10-batches-py/test_batch\n",
      "50000\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:  14.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:   6.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:   6.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:  10.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 10.1% (4500 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:   4.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:  16.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:  14.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:  12.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 9.3% (5423 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:   6.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:  20.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:  12.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:   8.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 9.3% (6579 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:  10.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:  12.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:  10.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:   8.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 9.8% (4556 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:  12.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:  14.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:  14.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:  12.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 10.3% (6263 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:  12.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:   8.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:  10.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:  12.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 8.8% (6806 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:   4.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:  12.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:  12.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:  14.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 9.1% (6279 / 1500)\n",
      "0\n",
      "Optimization Iteration:      1, Training Accuracy:  14.0%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Optimization Iteration:      6, Training Accuracy:  20.0%\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Optimization Iteration:     11, Training Accuracy:   8.0%\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Optimization Iteration:     16, Training Accuracy:  14.0%\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "1500\n",
      "cls_pred\n",
      "Accuracy on Validation-Set: 9.7% (4264 / 1500)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 8\n",
    "batch_size = 50\n",
    "\n",
    "#load dataset \n",
    "\n",
    "\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "\n",
    "#create index for validation and test set (5000 val, 5000 test)\n",
    "validation_index = 1500 # 500 (4 minutes) 1000 (7 min)\n",
    "x_validation = images_test[:validation_index, :, :, :]\n",
    "#x_validation = utils.preturb_images(x_validation)\n",
    "y_validation = labels_test[:validation_index, :]\n",
    "cls_validatoin = cls_test[:validation_index]\n",
    "\n",
    "x_test = images_test[validation_index:10000, :, :, :]\n",
    "#x_test = utils.preturb_images(x_test)\n",
    "y_test = labels_test[validation_index:10000]\n",
    "\n",
    "\n",
    "\n",
    "print(_num_images_train)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(20): #for computer power issues \n",
    "            X_batch, y_batch = random_batch(images_train,batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y_true: y_batch})\n",
    "            \n",
    "            print(iteration)\n",
    "            if iteration % 5 == 0:\n",
    "                # Calculate the accuracy on the training-set.\n",
    "                acc_train = accuracy.eval(feed_dict={X: X_batch, y_true: y_batch})\n",
    "                # Message for printing.\n",
    "                msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "                # Print it.\n",
    "                print(msg.format(iteration + 1, acc_train))\n",
    "        \n",
    "        print(len(x_validation))\n",
    "        correct, cls_pred = predict_cls(x_validation, y_validation, cls_validatoin)\n",
    "        print(\"cls_pred\")\n",
    "        classification_acc = correct.mean()\n",
    "        num_correct = cls_pred.sum()\n",
    "        msg = \"Accuracy on Validation-Set: {0:.1%} ({1} / {2})\"\n",
    "        print(msg.format(classification_acc, num_correct, len(x_validation)))\n",
    "        #print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "    \n",
    "    #falta provar el test\n",
    "\n",
    "    save_path = saver.save(sess, \"modelsave/conv1000val.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 hours  9.7"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowdos]",
   "language": "python",
   "name": "conda-env-tensorflowdos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
