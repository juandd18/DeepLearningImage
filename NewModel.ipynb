{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import image_embedding\n",
    "import image_processing\n",
    "import inputs as input_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShowAndTellModel(object):\n",
    "    \"\"\"Image-to-text implementation based on http://arxiv.org/abs/1411.4555.\n",
    "      \"Show and Tell: A Neural Image Caption Generator\"\n",
    "      Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, mode, train_inception=False):\n",
    "        \"\"\"Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "          mode: \"train\", \"eval\" or \"inference\".\n",
    "          train_inception: Whether the inception submodel variables are trainable.\n",
    "        \"\"\"\n",
    "        assert mode in [\"train\", \"eval\", \"inference\"]\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.train_inception = train_inception\n",
    "\n",
    "        # Reader for the input data.\n",
    "        self.reader = tf.TFRecordReader()\n",
    "\n",
    "        # To match the \"Show and Tell\" paper we initialize all variables with a\n",
    "        # random uniform initializer.\n",
    "        #------CODE FROM im2txt------\n",
    "        #self.initializer = tf.random_uniform_initializer(\n",
    "        #    minval=-self.config.initializer_scale,\n",
    "        #    maxval=self.config.initializer_scale)\n",
    "        \n",
    "        #He initialization \n",
    "        self.initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        \n",
    "        # A float32 Tensor with shape [batch_size, height, width, channels].\n",
    "        self.images = None\n",
    "\n",
    "        # An int32 Tensor with shape [batch_size, padded_length].\n",
    "        self.input_seqs = None\n",
    "\n",
    "        # An int32 Tensor with shape [batch_size, padded_length].\n",
    "        self.target_seqs = None\n",
    "\n",
    "        # An int32 0/1 Tensor with shape [batch_size, padded_length].\n",
    "        self.input_mask = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size, embedding_size].\n",
    "        self.image_embeddings = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size, padded_length, embedding_size].\n",
    "        self.seq_embeddings = None\n",
    "\n",
    "        # A float32 scalar Tensor; the total loss for the trainer to optimize.\n",
    "        self.total_loss = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size * padded_length].\n",
    "        self.target_cross_entropy_losses = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size * padded_length].\n",
    "        self.target_cross_entropy_loss_weights = None\n",
    "\n",
    "        # Collection of variables from the inception submodel.\n",
    "        self.inception_variables = []\n",
    "\n",
    "        # Function to restore the inception submodel from checkpoint.\n",
    "        self.init_fn = None\n",
    "\n",
    "        # Global step Tensor.\n",
    "        self.global_step = None\n",
    "\n",
    "    def is_training(self):\n",
    "        \"\"\"Returns true if the model is built for training mode.\"\"\"\n",
    "        return self.mode == \"train\"\n",
    "\n",
    "    def process_image(self, encoded_image, thread_id=0):\n",
    "        \"\"\"Decodes and processes an image string.\n",
    "        Args:\n",
    "          encoded_image: A scalar string Tensor; the encoded image.\n",
    "          thread_id: Preprocessing thread id used to select the ordering of color\n",
    "            distortions.\n",
    "        Returns:\n",
    "          A float32 Tensor of shape [height, width, 3]; the processed image.\n",
    "        \"\"\"\n",
    "        return image_processing.process_image(encoded_image,\n",
    "                                              is_training=self.is_training(),\n",
    "                                              height=self.config.image_height,\n",
    "                                              width=self.config.image_width,\n",
    "                                              thread_id=thread_id,\n",
    "                                              image_format=self.config.image_format)\n",
    "\n",
    "    def build_inputs(self):\n",
    "        \"\"\"Input prefetching, preprocessing and batching.\n",
    "        Outputs:\n",
    "          self.images\n",
    "          self.input_seqs\n",
    "          self.target_seqs (training and eval only)\n",
    "          self.input_mask (training and eval only)\n",
    "        \"\"\"\n",
    "        if self.mode == \"inference\":\n",
    "            # In inference mode, images and inputs are fed via placeholders.\n",
    "            image_feed = tf.placeholder(dtype=tf.string, shape=[], name=\"image_feed\")\n",
    "            input_feed = tf.placeholder(dtype=tf.int64,\n",
    "                                          shape=[None],  # batch_size\n",
    "                                          name=\"input_feed\")\n",
    "\n",
    "            # Process image and insert batch dimensions.\n",
    "            images = tf.expand_dims(self.process_image(image_feed), 0)\n",
    "            input_seqs = tf.expand_dims(input_feed, 1)\n",
    "\n",
    "            # No target sequences or input mask in inference mode.\n",
    "            target_seqs = None\n",
    "            input_mask = None\n",
    "        else:\n",
    "            # Prefetch serialized SequenceExample protos.\n",
    "            input_queue = input_ops.prefetch_input_data(\n",
    "                  self.reader,\n",
    "                  self.config.input_file_pattern,\n",
    "                  is_training=self.is_training(),\n",
    "                  batch_size=self.config.batch_size,\n",
    "                  values_per_shard=self.config.values_per_input_shard,\n",
    "                  input_queue_capacity_factor=self.config.input_queue_capacity_factor,\n",
    "                  num_reader_threads=self.config.num_input_reader_threads)\n",
    "\n",
    "            # Image processing and random distortion. Split across multiple threads\n",
    "            # with each thread applying a slightly different distortion.\n",
    "            assert self.config.num_preprocess_threads % 2 == 0\n",
    "            images_and_captions = []\n",
    "            for thread_id in range(self.config.num_preprocess_threads):\n",
    "                serialized_sequence_example = input_queue.dequeue()\n",
    "                encoded_image, caption = input_ops.parse_sequence_example(\n",
    "                    serialized_sequence_example,\n",
    "                    image_feature=self.config.image_feature_name,\n",
    "                    caption_feature=self.config.caption_feature_name)\n",
    "                image = self.process_image(encoded_image, thread_id=thread_id)\n",
    "                images_and_captions.append([image, caption])\n",
    "\n",
    "              # Batch inputs.\n",
    "            queue_capacity = (2 * self.config.num_preprocess_threads *\n",
    "                                self.config.batch_size)\n",
    "            images, input_seqs, target_seqs, input_mask = (\n",
    "                  input_ops.batch_with_dynamic_pad(images_and_captions,\n",
    "                                                   batch_size=self.config.batch_size,\n",
    "                                                   queue_capacity=queue_capacity))\n",
    "\n",
    "        self.images = images\n",
    "        self.input_seqs = input_seqs\n",
    "        self.target_seqs = target_seqs\n",
    "        self.input_mask = input_mask\n",
    "\n",
    "    def build_image_embeddings(self):\n",
    "       \n",
    "    \n",
    "        #create own convolutional \n",
    "    \n",
    "        inception_output = image_embedding.inception_v3(\n",
    "            self.images,\n",
    "            trainable=self.train_inception,\n",
    "            is_training=self.is_training())\n",
    "        self.inception_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\")\n",
    "\n",
    "        # Map inception output into embedding space.\n",
    "        with tf.variable_scope(\"image_embedding\") as scope:\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(\n",
    "                  inputs=inception_output,\n",
    "                  num_outputs=self.config.embedding_size,\n",
    "                  activation_fn=None,\n",
    "                  weights_initializer=self.initializer,\n",
    "                  biases_initializer=None,\n",
    "                  scope=scope)\n",
    "\n",
    "        # Save the embedding size in the graph.\n",
    "        tf.constant(self.config.embedding_size, name=\"embedding_size\")\n",
    "\n",
    "        self.image_embeddings = image_embeddings\n",
    "\n",
    "    def build_seq_embeddings(self):\n",
    "        \"\"\"Builds the input sequence embeddings.\n",
    "        Inputs:\n",
    "          self.input_seqs\n",
    "        Outputs:\n",
    "          self.seq_embeddings\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"seq_embedding\"), tf.device(\"/cpu:0\"):\n",
    "            embedding_map = tf.get_variable(\n",
    "                  name=\"map\",\n",
    "                  shape=[self.config.vocab_size, self.config.embedding_size],\n",
    "                  initializer=self.initializer)\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_map, self.input_seqs)\n",
    "\n",
    "        self.seq_embeddings = seq_embeddings\n",
    "\n",
    "    def build_model(self):\n",
    "       \n",
    "\n",
    "    def setup_inception_initializer(self):\n",
    "    \n",
    "    def setup_global_step(self):\n",
    "        \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "        global_step = tf.Variable(\n",
    "            initial_value=0,\n",
    "            name=\"global_step\",\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "        self.global_step = global_step\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Creates all ops for training and evaluation.\"\"\"\n",
    "        self.build_inputs()\n",
    "        self.build_image_embeddings()\n",
    "        self.build_seq_embeddings()\n",
    "        #self.build_model()\n",
    "        #self.setup_inception_initializer()\n",
    "        #self.setup_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    \n",
    "    #image weight 299\n",
    "    #image hight 299\n",
    "    \n",
    "    \n",
    "    \n",
    "    weights = { \n",
    "    # 5x5 conv, 3 input, 64 outputs \n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 64])), \n",
    "    # 5x5 conv, 64 inputs, 64 outputs \n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 64, 192])), \n",
    "    # 5x5 conv, 192 inputs, 192 outputs \n",
    "    'wc3': tf.Variable(tf.random_normal([5, 5, 192, 1024])), \n",
    "        \n",
    "    'wd1': tf.Variable(tf.random_normal([37 * 37 * 1024,1024])),\n",
    "        \n",
    "    # 1024 inputs, 10 outputs (class prediction) \n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes])) \n",
    "    } \n",
    "\n",
    "    biases = { \n",
    "    'bc1': tf.Variable(tf.random_normal([64])), \n",
    "    'bc2': tf.Variable(tf.random_normal([192])), \n",
    "    'bc3': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024]))\n",
    "    'out': tf.Variable(tf.random_normal([n_classes])) \n",
    "    } \n",
    "    \n",
    "    #(W−F+2P)/S + 1\n",
    "    #299 - 5 + 2*1  / 1 +1 = 297\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        # apply zero padding image of size 301 x 301 x 3\n",
    "        # 5x5 conv, 3 input, 64 outputs = 299 X 299 x 64\n",
    "        conv = tf.nn.conv2d(images, weights['wc1'], [1, 1, 1, 1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases['bc1'])\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "    \n",
    "    #apply zero padding conv1 of size 301 x 301 x 64\n",
    "    # pool 3 x 3 = 149 x 149 x 64  (296/2) + 1 .. W2=(W1−F)/S + 1\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],padding='SAME', name='pool1')\n",
    "    \n",
    "    #same size 149 x 149 x 64\n",
    "    # norm1 \n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        #apply zero padding image of size 151 x 151 x 64\n",
    "        # 5x5 conv, 64 input, 192 outputs = 149 X 149 x ????  could be any value\n",
    "        conv = tf.nn.conv2d(norm1, weights['wc2'], [1, 1, 1, 1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases['bc2'])\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                        name='norm2')\n",
    "    \n",
    "    #apply zero padding conv1 of size 151 x 151 x 192\n",
    "    # pool 3 x 3 = 74 x 74 x 192\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                             strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    # conv3\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        #apply zero padding image of size 76 x 76 x 192\n",
    "        # 5x5 conv, 192 input, 1024 outputs = 74 X 74 x 1024  could be any value???\n",
    "        conv = tf.nn.conv2d(norm1, weights['wc3'], [1, 1, 1, 1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases['bc3'])\n",
    "        conv3 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv3)\n",
    "\n",
    "    # norm3\n",
    "    norm3 = tf.nn.lrn(conv3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm3')\n",
    "    \n",
    "    # pool 2 x 2 = 37 x 37 x 1024\n",
    "    # pool3\n",
    "    pool3 = tf.nn.max_pool(norm3, ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1], padding='SAME', name='pool3')\n",
    "    \n",
    "    \n",
    "    # local3\n",
    "    with tf.variable_scope('fulllayer1') as scope:\n",
    "        # Fully connected layer \n",
    "        # Reshape conv3 output to fit fully connected layer input \n",
    "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) \n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1']) \n",
    "        fc1 = tf.nn.relu(fc1) \n",
    "        # Apply Dropout \n",
    "        fc1 = tf.nn.dropout(fc1, dropout) \n",
    "\n",
    "        # Output, class prediction \n",
    "        local3 = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "        \n",
    "        \n",
    "       \n",
    "    # linear layer(WX + b),\n",
    "    # We don't apply softmax here because\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "    # and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],stddev=1/192.0, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local3, weights), biases, name=scope.name)\n",
    "        _activation_summary(softmax_linear)\n",
    "\n",
    "    softmax_linear\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow12]",
   "language": "python",
   "name": "conda-env-tensorflow12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
