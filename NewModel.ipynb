{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import image_embedding\n",
    "import image_processing\n",
    "import inputs as input_ops\n",
    "\n",
    "import numpy as np\n",
    "import utils_project \n",
    "#import utils_caltech\n",
    "import configuration\n",
    "import cifar10\n",
    "from cifar10 import img_size, num_channels, num_classes,_num_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(image_train,train_batch_size):\n",
    "    # Number of images in the training-set.\n",
    "    num_images = len(images_train)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = images_train[idx, :, :, :]\n",
    "    y_batch = labels_train[idx, :]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_cls(images, labels, cls_true):\n",
    "    # Number of images.\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_images:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_images)\n",
    "\n",
    "        # Create a feed-dict with the images and labels\n",
    "        # between index i and j.\n",
    "        feed_dict = {x: images[i:j, :],\n",
    "                     y_true: labels[i:j, :]}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    return correct, cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"convol_dnn/leak_relu_1:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "utils = utils_project\n",
    "\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "with tf.name_scope(\"convol_dnn\"):\n",
    "    \n",
    "    #shape formula W2=(W1−F+2P)/S+1 \n",
    "    num_input_channels=3\n",
    "    num_inputs = (32*32*3)\n",
    "    num_filters = 64\n",
    "    kernel_size = 5\n",
    "    layern1 = utils.conv_layer(X,\"layer1\",\n",
    "                                       use_pooling=False,\n",
    "                                       dropout = False,\n",
    "                                       activation=\"leak_relu\",\n",
    "                                       num_inputs=num_inputs,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       kernel_size=kernel_size,\n",
    "                                       num_filters=num_filters,\n",
    "                                       padding = 'SAME')\n",
    "    \n",
    "    #num_inputs = 32*32*64\n",
    "    #pool formula (W1−F)/S+1  \n",
    "    #pool shape from utils_project.conv_layer ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1]\n",
    "    layern2 = utils_project.conv_layer(layern1,\"layer2\",use_pooling=True,\n",
    "                                       dropout=True,\n",
    "                                       activation=\"leak_relu\",\n",
    "                                       num_inputs=32*32*64,\n",
    "                                       num_input_channels=64,\n",
    "                                       kernel_size=5,\n",
    "                                       num_filters=128,\n",
    "                                       padding = 'SAME')\n",
    "    #output shape [none, 16,16,128]\n",
    "    \n",
    "    \n",
    "    layern3 = utils_project.conv_layer(layern2,\"layer3\",use_pooling=True,\n",
    "                                       dropout=True,\n",
    "                                       activation=\"leak_relu\",\n",
    "                                       num_inputs=16*16*128,\n",
    "                                       num_input_channels=128,\n",
    "                                       kernel_size=5,\n",
    "                                       num_filters=328,\n",
    "                                       padding = 'SAME')\n",
    "    #output shape [none, 8,8,328]\n",
    "    \n",
    "    #[None,8*8*328]\n",
    "    flatten_tensor,num_features = utils_project.flatten_layer(layern3)\n",
    "    \n",
    "    layer_fc1 = utils_project.new_fc_layer(input=flatten_tensor,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=328,\n",
    "                         activation=\"leak_relu\")\n",
    "    \n",
    "    layer_fc2 = utils_project.new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=328,\n",
    "                         num_outputs=10,\n",
    "                         activation=\"leak_relu\")\n",
    "\n",
    "    \n",
    "    y_pred = tf.nn.softmax(layer_fc2)\n",
    "    \n",
    "    y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    print(layer_fc2)\n",
    "    #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(layer_fc2, y_true)\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(layer_fc2, y_true)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/cifar-10-batches-py/test_batch\n",
      "50000\n",
      "0\n",
      "0.136\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "\n",
    "#load dataset \n",
    "\n",
    "\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "\n",
    "\n",
    "\n",
    "print(_num_images_train)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1):\n",
    "            X_batch, y_batch = random_batch(images_train,batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y_true: y_batch})\n",
    "            # Print status every 100 iterations.\n",
    "            if i % 100 == 0:\n",
    "                # Calculate the accuracy on the training-set.\n",
    "                acc_train = accuracy.eval(feed_dict={X: X_batch, y_true: y_batch})\n",
    "                # Message for printing.\n",
    "                msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "                # Print it.\n",
    "                print(msg.format(i + 1, acc_train))\n",
    "            \n",
    "    \n",
    "        acc_test = accuracy.eval(feed_dict={X: images_test, y_true: labels_test})\n",
    "        #print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow12_1]",
   "language": "python",
   "name": "conda-env-tensorflow12_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
