{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import image_embedding\n",
    "import image_processing\n",
    "import inputs as input_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShowAndTellModel(object):\n",
    "    \"\"\"Image-to-text implementation based on http://arxiv.org/abs/1411.4555.\n",
    "      \"Show and Tell: A Neural Image Caption Generator\"\n",
    "      Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, mode, train_inception=False):\n",
    "        \"\"\"Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "          mode: \"train\", \"eval\" or \"inference\".\n",
    "          train_inception: Whether the inception submodel variables are trainable.\n",
    "        \"\"\"\n",
    "        assert mode in [\"train\", \"eval\", \"inference\"]\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.train_inception = train_inception\n",
    "\n",
    "        # Reader for the input data.\n",
    "        self.reader = tf.TFRecordReader()\n",
    "\n",
    "        # To match the \"Show and Tell\" paper we initialize all variables with a\n",
    "        # random uniform initializer.\n",
    "        #------CODE FROM im2txt------\n",
    "        #self.initializer = tf.random_uniform_initializer(\n",
    "        #    minval=-self.config.initializer_scale,\n",
    "        #    maxval=self.config.initializer_scale)\n",
    "        \n",
    "        #He initialization \n",
    "        self.initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        \n",
    "        # A float32 Tensor with shape [batch_size, height, width, channels].\n",
    "        self.images = None\n",
    "\n",
    "        # An int32 Tensor with shape [batch_size, padded_length].\n",
    "        self.input_seqs = None\n",
    "\n",
    "        # An int32 Tensor with shape [batch_size, padded_length].\n",
    "        self.target_seqs = None\n",
    "\n",
    "        # An int32 0/1 Tensor with shape [batch_size, padded_length].\n",
    "        self.input_mask = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size, embedding_size].\n",
    "        self.image_embeddings = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size, padded_length, embedding_size].\n",
    "        self.seq_embeddings = None\n",
    "\n",
    "        # A float32 scalar Tensor; the total loss for the trainer to optimize.\n",
    "        self.total_loss = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size * padded_length].\n",
    "        self.target_cross_entropy_losses = None\n",
    "\n",
    "        # A float32 Tensor with shape [batch_size * padded_length].\n",
    "        self.target_cross_entropy_loss_weights = None\n",
    "\n",
    "        # Collection of variables from the inception submodel.\n",
    "        self.inception_variables = []\n",
    "\n",
    "        # Function to restore the inception submodel from checkpoint.\n",
    "        self.init_fn = None\n",
    "\n",
    "        # Global step Tensor.\n",
    "        self.global_step = None\n",
    "\n",
    "    def is_training(self):\n",
    "        \"\"\"Returns true if the model is built for training mode.\"\"\"\n",
    "        return self.mode == \"train\"\n",
    "\n",
    "    def process_image(self, encoded_image, thread_id=0):\n",
    "        \"\"\"Decodes and processes an image string.\n",
    "        Args:\n",
    "          encoded_image: A scalar string Tensor; the encoded image.\n",
    "          thread_id: Preprocessing thread id used to select the ordering of color\n",
    "            distortions.\n",
    "        Returns:\n",
    "          A float32 Tensor of shape [height, width, 3]; the processed image.\n",
    "        \"\"\"\n",
    "        return image_processing.process_image(encoded_image,\n",
    "                                              is_training=self.is_training(),\n",
    "                                              height=self.config.image_height,\n",
    "                                              width=self.config.image_width,\n",
    "                                              thread_id=thread_id,\n",
    "                                              image_format=self.config.image_format)\n",
    "\n",
    "    def build_inputs(self):\n",
    "        \"\"\"Input prefetching, preprocessing and batching.\n",
    "        Outputs:\n",
    "          self.images\n",
    "          self.input_seqs\n",
    "          self.target_seqs (training and eval only)\n",
    "          self.input_mask (training and eval only)\n",
    "        \"\"\"\n",
    "        if self.mode == \"inference\":\n",
    "            # In inference mode, images and inputs are fed via placeholders.\n",
    "            image_feed = tf.placeholder(dtype=tf.string, shape=[], name=\"image_feed\")\n",
    "            input_feed = tf.placeholder(dtype=tf.int64,\n",
    "                                          shape=[None],  # batch_size\n",
    "                                          name=\"input_feed\")\n",
    "\n",
    "            # Process image and insert batch dimensions.\n",
    "            images = tf.expand_dims(self.process_image(image_feed), 0)\n",
    "            input_seqs = tf.expand_dims(input_feed, 1)\n",
    "\n",
    "            # No target sequences or input mask in inference mode.\n",
    "            target_seqs = None\n",
    "            input_mask = None\n",
    "        else:\n",
    "            # Prefetch serialized SequenceExample protos.\n",
    "            input_queue = input_ops.prefetch_input_data(\n",
    "                  self.reader,\n",
    "                  self.config.input_file_pattern,\n",
    "                  is_training=self.is_training(),\n",
    "                  batch_size=self.config.batch_size,\n",
    "                  values_per_shard=self.config.values_per_input_shard,\n",
    "                  input_queue_capacity_factor=self.config.input_queue_capacity_factor,\n",
    "                  num_reader_threads=self.config.num_input_reader_threads)\n",
    "\n",
    "            # Image processing and random distortion. Split across multiple threads\n",
    "            # with each thread applying a slightly different distortion.\n",
    "            assert self.config.num_preprocess_threads % 2 == 0\n",
    "            images_and_captions = []\n",
    "            for thread_id in range(self.config.num_preprocess_threads):\n",
    "                serialized_sequence_example = input_queue.dequeue()\n",
    "                encoded_image, caption = input_ops.parse_sequence_example(\n",
    "                    serialized_sequence_example,\n",
    "                    image_feature=self.config.image_feature_name,\n",
    "                    caption_feature=self.config.caption_feature_name)\n",
    "                image = self.process_image(encoded_image, thread_id=thread_id)\n",
    "                images_and_captions.append([image, caption])\n",
    "\n",
    "              # Batch inputs.\n",
    "            queue_capacity = (2 * self.config.num_preprocess_threads *\n",
    "                                self.config.batch_size)\n",
    "            images, input_seqs, target_seqs, input_mask = (\n",
    "                  input_ops.batch_with_dynamic_pad(images_and_captions,\n",
    "                                                   batch_size=self.config.batch_size,\n",
    "                                                   queue_capacity=queue_capacity))\n",
    "\n",
    "        self.images = images\n",
    "        self.input_seqs = input_seqs\n",
    "        self.target_seqs = target_seqs\n",
    "        self.input_mask = input_mask\n",
    "\n",
    "    def build_image_embeddings(self):\n",
    "        \"\"\"Builds the image model subgraph and generates image embeddings.\n",
    "        Inputs:\n",
    "          self.images\n",
    "        Outputs:\n",
    "          self.image_embeddings\n",
    "        \"\"\"\n",
    "        inception_output = image_embedding.inception_v3(\n",
    "            self.images,\n",
    "            trainable=self.train_inception,\n",
    "            is_training=self.is_training())\n",
    "        self.inception_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\")\n",
    "\n",
    "        # Map inception output into embedding space.\n",
    "        with tf.variable_scope(\"image_embedding\") as scope:\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(\n",
    "                  inputs=inception_output,\n",
    "                  num_outputs=self.config.embedding_size,\n",
    "                  activation_fn=None,\n",
    "                  weights_initializer=self.initializer,\n",
    "                  biases_initializer=None,\n",
    "                  scope=scope)\n",
    "\n",
    "        # Save the embedding size in the graph.\n",
    "        tf.constant(self.config.embedding_size, name=\"embedding_size\")\n",
    "\n",
    "        self.image_embeddings = image_embeddings\n",
    "\n",
    "    def build_seq_embeddings(self):\n",
    "        \"\"\"Builds the input sequence embeddings.\n",
    "        Inputs:\n",
    "          self.input_seqs\n",
    "        Outputs:\n",
    "          self.seq_embeddings\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"seq_embedding\"), tf.device(\"/cpu:0\"):\n",
    "            embedding_map = tf.get_variable(\n",
    "                  name=\"map\",\n",
    "                  shape=[self.config.vocab_size, self.config.embedding_size],\n",
    "                  initializer=self.initializer)\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_map, self.input_seqs)\n",
    "\n",
    "        self.seq_embeddings = seq_embeddings\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the model.\n",
    "        Inputs:\n",
    "          self.image_embeddings\n",
    "          self.seq_embeddings\n",
    "          self.target_seqs (training and eval only)\n",
    "          self.input_mask (training and eval only)\n",
    "        Outputs:\n",
    "          self.total_loss (training and eval only)\n",
    "          self.target_cross_entropy_losses (training and eval only)\n",
    "          self.target_cross_entropy_loss_weights (training and eval only)\n",
    "        \"\"\"\n",
    "        # This LSTM cell has biases and outputs tanh(new_c) * sigmoid(o), but the\n",
    "        # modified LSTM in the \"Show and Tell\" paper has no biases and outputs\n",
    "        # new_c * sigmoid(o).\n",
    "        # TEST TO USE  tf.nn.rnn_cell.GRUCell(num_units=n_neurons) \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "            num_units=self.config.num_lstm_units, state_is_tuple=True)\n",
    "        if self.mode == \"train\":\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                  lstm_cell,\n",
    "                  input_keep_prob=self.config.lstm_dropout_keep_prob,\n",
    "                  output_keep_prob=self.config.lstm_dropout_keep_prob)\n",
    "\n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "          # Feed the image embeddings to set the initial LSTM state.\n",
    "            zero_state = lstm_cell.zero_state(\n",
    "                  batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(self.image_embeddings, zero_state)\n",
    "\n",
    "              # Allow the LSTM variables to be reused.\n",
    "            lstm_scope.reuse_variables()\n",
    "\n",
    "            if self.mode == \"inference\":\n",
    "                # In inference mode, use concatenated states for convenient feeding and\n",
    "                # fetching.\n",
    "                tf.concat(1, initial_state, name=\"initial_state\")\n",
    "\n",
    "                # Placeholder for feeding a batch of concatenated states.\n",
    "                state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                            shape=[None, sum(lstm_cell.state_size)],\n",
    "                                            name=\"state_feed\")\n",
    "                state_tuple = tf.split(1, 2, state_feed)\n",
    "\n",
    "                # Run a single LSTM step.\n",
    "                lstm_outputs, state_tuple = lstm_cell(\n",
    "                    inputs=tf.squeeze(self.seq_embeddings, squeeze_dims=[1]),\n",
    "                    state=state_tuple)\n",
    "\n",
    "                # Concatentate the resulting state.\n",
    "                tf.concat(1, state_tuple, name=\"state\")\n",
    "            else:\n",
    "                # Run the batch of sequence embeddings through the LSTM.\n",
    "                sequence_length = tf.reduce_sum(self.input_mask, 1)\n",
    "                lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                                    inputs=self.seq_embeddings,\n",
    "                                                    sequence_length=sequence_length,\n",
    "                                                    initial_state=initial_state,\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    scope=lstm_scope)\n",
    "\n",
    "        # Stack batches vertically.\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                  inputs=lstm_outputs,\n",
    "                  num_outputs=self.config.vocab_size,\n",
    "                  activation_fn=None,\n",
    "                  weights_initializer=self.initializer,\n",
    "                  scope=logits_scope)\n",
    "\n",
    "        if self.mode == \"inference\":\n",
    "            tf.nn.softmax(logits, name=\"softmax\")\n",
    "        else:\n",
    "            targets = tf.reshape(self.target_seqs, [-1])\n",
    "            weights = tf.to_float(tf.reshape(self.input_mask, [-1]))\n",
    "\n",
    "            # Compute losses.\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets)\n",
    "            batch_loss = tf.div(tf.reduce_sum(tf.mul(losses, weights)),\n",
    "                                  tf.reduce_sum(weights),\n",
    "                                  name=\"batch_loss\")\n",
    "            tf.contrib.losses.add_loss(batch_loss)\n",
    "            total_loss = tf.contrib.losses.get_total_loss()\n",
    "\n",
    "            # Add summaries.\n",
    "            tf.summary.scalar(\"batch_loss\", batch_loss)\n",
    "            tf.summary.scalar(\"total_loss\", total_loss)\n",
    "            for var in tf.trainable_variables():\n",
    "                tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "            self.total_loss = total_loss\n",
    "            self.target_cross_entropy_losses = losses  # Used in evaluation.\n",
    "            self.target_cross_entropy_loss_weights = weights  # Used in evaluation.\n",
    "\n",
    "    def setup_inception_initializer(self):\n",
    "        \"\"\"Sets up the function to restore inception variables from checkpoint.\"\"\"\n",
    "        if self.mode != \"inference\":\n",
    "            # Restore inception variables only.\n",
    "            saver = tf.train.Saver(self.inception_variables)\n",
    "\n",
    "            def restore_fn(sess):\n",
    "                tf.logging.info(\"Restoring Inception variables from checkpoint file %s\",\n",
    "                                \"/Users/juandavid/Documents/machine-learning-master/projects/DeepLearningProjectImage/inception_v3.ckpt\")\n",
    "                saver.restore(sess, \"/Users/juandavid/Documents/machine-learning-master/projects/DeepLearningProjectImage/inception_v3.ckpt\")\n",
    "\n",
    "            self.init_fn = restore_fn\n",
    "\n",
    "    def setup_global_step(self):\n",
    "        \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "        global_step = tf.Variable(\n",
    "            initial_value=0,\n",
    "            name=\"global_step\",\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "        self.global_step = global_step\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Creates all ops for training and evaluation.\"\"\"\n",
    "        self.build_inputs()\n",
    "        self.build_image_embeddings()\n",
    "        self.build_seq_embeddings()\n",
    "        self.build_model()\n",
    "        self.setup_inception_initializer()\n",
    "        self.setup_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    \"\"\"Wrapper class for model hyperparameters.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Sets the default model hyperparameters.\"\"\"\n",
    "        # File pattern of sharded TFRecord file containing SequenceExample protos.\n",
    "        # Must be provided in training and evaluation modes.\n",
    "        self.input_file_pattern = None\n",
    "\n",
    "        # Image format (\"jpeg\" or \"png\").\n",
    "        self.image_format = \"jpeg\"\n",
    "\n",
    "        # Approximate number of values per input shard. Used to ensure sufficient\n",
    "        # mixing between shards in training.\n",
    "        self.values_per_input_shard = 2300\n",
    "        # Minimum number of shards to keep in the input queue.\n",
    "        self.input_queue_capacity_factor = 2\n",
    "        # Number of threads for prefetching SequenceExample protos.\n",
    "        self.num_input_reader_threads = 1\n",
    "\n",
    "        # Name of the SequenceExample context feature containing image data.\n",
    "        self.image_feature_name = \"image/data\"\n",
    "        # Name of the SequenceExample feature list containing integer captions.\n",
    "        self.caption_feature_name = \"image/caption_ids\"\n",
    "\n",
    "        # Number of unique words in the vocab (plus 1, for <UNK>).\n",
    "        # The default value is larger than the expected actual vocab size to allow\n",
    "        # for differences between tokenizer versions used in preprocessing. There is\n",
    "        # no harm in using a value greater than the actual vocab size, but using a\n",
    "        # value less than the actual vocab size will result in an error.\n",
    "        self.vocab_size = 12000\n",
    "\n",
    "        # Number of threads for image preprocessing. Should be a multiple of 2.\n",
    "        self.num_preprocess_threads = 4\n",
    "\n",
    "        # Batch size.\n",
    "        self.batch_size = 32\n",
    "\n",
    "        # File containing an Inception v3 checkpoint to initialize the variables\n",
    "        # of the Inception model. Must be provided when starting training for the\n",
    "        # first time.\n",
    "        self.inception_checkpoint_file = None\n",
    "\n",
    "        # Dimensions of Inception v3 input images.\n",
    "        self.image_height = 299\n",
    "        self.image_width = 299\n",
    "\n",
    "        # Scale used to initialize model variables.\n",
    "        self.initializer_scale = 0.08\n",
    "\n",
    "        # LSTM input and output dimensionality, respectively.\n",
    "        self.embedding_size = 512\n",
    "        self.num_lstm_units = 512\n",
    "\n",
    "        # If < 1.0, the dropout keep probability applied to LSTM variables.\n",
    "        self.lstm_dropout_keep_prob = 0.7\n",
    "\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    \"\"\"Wrapper class for training hyperparameters.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Sets the default training hyperparameters.\"\"\"\n",
    "        # Number of examples per epoch of training data.\n",
    "        self.num_examples_per_epoch = 586363\n",
    "\n",
    "        \n",
    "        # Learning rate for the initial phase of training.\n",
    "        #----CODE im2txt----\n",
    "        #self.initial_learning_rate = 2.0\n",
    "        self.initial_learning_rate = 0.05\n",
    "        \n",
    "        self.learning_rate_decay_factor = 0.5\n",
    "        self.num_epochs_per_decay = 8.0\n",
    "        \n",
    "        # Optimizer for training the model.\n",
    "        #----CODE im2txt----\n",
    "        #self.optimizer = \"SGD\"\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.initial_learning_rate)\n",
    "\n",
    "        # Learning rate when fine tuning the Inception v3 parameters.\n",
    "        self.train_inception_learning_rate = 0.0005\n",
    "\n",
    "        # If not None, clip gradients to this value.\n",
    "        self.clip_gradients = 5.0\n",
    "\n",
    "        # How many model checkpoints to keep.\n",
    "        self.max_checkpoints_to_keep = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_inception =False\n",
    "#train_dir=\"outputImage/train\"\n",
    "\n",
    "def main(train_dir,train_inception):\n",
    "\n",
    "    model_config = ModelConfig()\n",
    "    model_config.input_file_pattern = \"outputImage/train-?????-of-00256\"\n",
    "    model_config.inception_checkpoint_file = \"inception_v3.ckpt\"\n",
    "    training_config = TrainingConfig()\n",
    "    log_every_n_steps = 2\n",
    "    number_of_steps = 1000000\n",
    "    \n",
    "    # Create training directory.\n",
    "    train_dir = train_dir\n",
    "    if not tf.gfile.IsDirectory(train_dir):\n",
    "        tf.logging.info(\"Creating training directory: %s\", train_dir)\n",
    "        tf.gfile.MakeDirs(train_dir)\n",
    "\n",
    "    # Build the TensorFlow graph.\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        # Build the model.\n",
    "        model = ShowAndTellModel(model_config, mode=\"train\", train_inception=train_inception)\n",
    "        model.build()\n",
    "\n",
    "        # Set up the learning rate.\n",
    "        learning_rate_decay_fn = None\n",
    "        if train_inception:\n",
    "            learning_rate = tf.constant(training_config.train_inception_learning_rate)\n",
    "        else:\n",
    "            learning_rate = tf.constant(training_config.initial_learning_rate)\n",
    "            #-----im2txt---- \n",
    "            #if training_config.learning_rate_decay_factor > 0:\n",
    "            #    num_batches_per_epoch = (training_config.num_examples_per_epoch /\n",
    "            #                             model_config.batch_size)\n",
    "            #    decay_steps = int(num_batches_per_epoch *\n",
    "            #                      training_config.num_epochs_per_decay)\n",
    "\n",
    "            #def _learning_rate_decay_fn(learning_rate, global_step):\n",
    "            #    return tf.train.exponential_decay(\n",
    "            #          learning_rate,\n",
    "            #          global_step,\n",
    "            #          decay_steps=decay_steps,\n",
    "            #          decay_rate=training_config.learning_rate_decay_factor,\n",
    "            #          staircase=True)\n",
    "\n",
    "            #learning_rate_decay_fn = _learning_rate_decay_fn\n",
    "\n",
    "        # Set up the training ops.\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=model.total_loss,\n",
    "            global_step=model.global_step,\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=training_config.optimizer,\n",
    "            clip_gradients=training_config.clip_gradients ) #,\n",
    "            #learning_rate_decay_fn=learning_rate_decay_fn)\n",
    "\n",
    "        # Set up the Saver for saving and restoring model checkpoints.\n",
    "        saver = tf.train.Saver(max_to_keep=training_config.max_checkpoints_to_keep)\n",
    "\n",
    "      # Run training.\n",
    "    tf.contrib.slim.learning.train(\n",
    "          train_op,\n",
    "          train_dir,\n",
    "          log_every_n_steps=log_every_n_steps,\n",
    "          graph=g,\n",
    "          global_step=model.global_step,\n",
    "          number_of_steps=number_of_steps,\n",
    "          init_fn=model.init_fn,\n",
    "          saver=saver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Prefetching values from 256 files matching outputImage/train-?????-of-00256\n",
      "WARNING:tensorflow:From inputs.py:121 in prefetch_input_data.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From image_processing.py:95 in image_summary.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From image_processing.py:95 in image_summary.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From image_processing.py:95 in image_summary.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From inputs.py:200 in batch_with_dynamic_pad.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From inputs.py:201 in batch_with_dynamic_pad.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From inputs.py:202 in batch_with_dynamic_pad.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py:344 in __init__.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "INFO:tensorflow:Restoring Inception variables from checkpoint file /Users/juandavid/Documents/machine-learning-master/projects/DeepLearningProjectImage/inception_v3.ckpt\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global step 2: loss = 8.3497 (24.00 sec/step)\n",
      "INFO:tensorflow:global step 4: loss = 15.5347 (22.28 sec/step)\n",
      "INFO:tensorflow:global step 6: loss = 11.3127 (28.25 sec/step)\n",
      "INFO:tensorflow:global step 8: loss = 8.2885 (21.31 sec/step)\n",
      "INFO:tensorflow:global step 10: loss = 7.2557 (21.87 sec/step)\n",
      "INFO:tensorflow:global step 12: loss = 7.4041 (22.16 sec/step)\n",
      "INFO:tensorflow:global step 14: loss = 6.6347 (22.27 sec/step)\n",
      "INFO:tensorflow:global step 16: loss = 6.4628 (22.16 sec/step)\n",
      "INFO:tensorflow:global step 18: loss = 6.1296 (22.11 sec/step)\n",
      "INFO:tensorflow:global step 20: loss = 6.0882 (22.44 sec/step)\n",
      "INFO:tensorflow:global step 22: loss = 5.8974 (22.62 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0383551\n",
      "INFO:tensorflow:global step 24: loss = 6.1231 (44.35 sec/step)\n",
      "INFO:tensorflow:global step 26: loss = 5.9882 (22.71 sec/step)\n",
      "INFO:tensorflow:global step 28: loss = 5.6491 (22.31 sec/step)\n",
      "INFO:tensorflow:global step 30: loss = 5.7743 (26.99 sec/step)\n",
      "INFO:tensorflow:global step 32: loss = 6.1020 (24.59 sec/step)\n",
      "INFO:tensorflow:global step 34: loss = 5.7367 (22.36 sec/step)\n",
      "INFO:tensorflow:global step 36: loss = 5.7688 (24.26 sec/step)\n",
      "INFO:tensorflow:global step 38: loss = 5.5714 (23.74 sec/step)\n",
      "INFO:tensorflow:global step 40: loss = 5.3529 (22.62 sec/step)\n",
      "INFO:tensorflow:global step 42: loss = 6.1936 (23.22 sec/step)\n",
      "INFO:tensorflow:global step 44: loss = 5.4366 (22.47 sec/step)\n",
      "INFO:tensorflow:global step 46: loss = 5.5138 (23.79 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0400066\n",
      "INFO:tensorflow:global step 48: loss = 5.4676 (29.17 sec/step)\n",
      "INFO:tensorflow:global step 50: loss = 5.2000 (22.50 sec/step)\n",
      "INFO:tensorflow:global step 52: loss = 5.8161 (24.88 sec/step)\n",
      "INFO:tensorflow:global step 54: loss = 5.6234 (22.92 sec/step)\n",
      "INFO:tensorflow:global step 56: loss = 5.4009 (22.60 sec/step)\n",
      "INFO:tensorflow:global step 58: loss = 5.2663 (22.83 sec/step)\n",
      "INFO:tensorflow:global step 60: loss = 5.5729 (29.59 sec/step)\n",
      "INFO:tensorflow:global step 62: loss = 5.1146 (30.30 sec/step)\n",
      "INFO:tensorflow:global step 64: loss = 5.4176 (29.88 sec/step)\n",
      "INFO:tensorflow:global step 66: loss = 4.8641 (27.65 sec/step)\n",
      "INFO:tensorflow:global step 68: loss = 5.4566 (28.17 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0382695\n",
      "INFO:tensorflow:global step 70: loss = 5.0289 (22.89 sec/step)\n",
      "INFO:tensorflow:global step 72: loss = 5.2794 (25.93 sec/step)\n",
      "INFO:tensorflow:global step 74: loss = 5.2903 (23.29 sec/step)\n",
      "INFO:tensorflow:global step 76: loss = 5.6239 (23.60 sec/step)\n",
      "INFO:tensorflow:global step 78: loss = 5.4478 (23.58 sec/step)\n",
      "INFO:tensorflow:global step 80: loss = 5.2289 (22.67 sec/step)\n",
      "INFO:tensorflow:global step 82: loss = 5.6548 (23.24 sec/step)\n",
      "INFO:tensorflow:global step 84: loss = 5.1307 (22.60 sec/step)\n",
      "INFO:tensorflow:global step 86: loss = 5.1124 (23.32 sec/step)\n",
      "INFO:tensorflow:global step 88: loss = 5.4242 (23.77 sec/step)\n",
      "INFO:tensorflow:global step 90: loss = 5.2667 (31.14 sec/step)\n",
      "INFO:tensorflow:global step 92: loss = 5.3427 (22.47 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0383772\n",
      "INFO:tensorflow:global step 94: loss = 5.2540 (37.30 sec/step)\n",
      "INFO:tensorflow:global step 96: loss = 4.8915 (22.50 sec/step)\n",
      "INFO:tensorflow:global step 98: loss = 5.3100 (22.43 sec/step)\n",
      "INFO:tensorflow:global step 100: loss = 5.0652 (22.93 sec/step)\n",
      "INFO:tensorflow:global step 102: loss = 4.9816 (22.98 sec/step)\n",
      "INFO:tensorflow:global step 104: loss = 5.1335 (27.50 sec/step)\n",
      "INFO:tensorflow:global step 106: loss = 5.4812 (26.05 sec/step)\n",
      "INFO:tensorflow:global step 108: loss = 4.8318 (23.91 sec/step)\n",
      "INFO:tensorflow:global step 110: loss = 4.9788 (23.49 sec/step)\n",
      "INFO:tensorflow:global step 112: loss = 5.0922 (26.24 sec/step)\n",
      "INFO:tensorflow:global step 114: loss = 5.5683 (24.94 sec/step)\n",
      "INFO:tensorflow:global step 116: loss = 5.1187 (24.97 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.039989\n",
      "INFO:tensorflow:global step 118: loss = 5.7159 (59.98 sec/step)\n",
      "INFO:tensorflow:global step 120: loss = 5.9947 (25.65 sec/step)\n",
      "INFO:tensorflow:global step 122: loss = 5.3536 (31.03 sec/step)\n",
      "INFO:tensorflow:global step 124: loss = 4.9424 (25.44 sec/step)\n",
      "INFO:tensorflow:global step 126: loss = 5.6167 (25.67 sec/step)\n",
      "INFO:tensorflow:global step 128: loss = 5.5914 (28.13 sec/step)\n",
      "INFO:tensorflow:global step 130: loss = 4.9639 (23.99 sec/step)\n",
      "INFO:tensorflow:global step 132: loss = 5.0109 (25.61 sec/step)\n",
      "INFO:tensorflow:global step 134: loss = 5.0022 (24.93 sec/step)\n",
      "INFO:tensorflow:global step 136: loss = 5.0654 (25.88 sec/step)\n",
      "INFO:tensorflow:global step 138: loss = 4.6622 (23.56 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0349795\n",
      "INFO:tensorflow:global step 140: loss = 5.3090 (47.49 sec/step)\n",
      "INFO:tensorflow:global step 142: loss = 4.7156 (24.71 sec/step)\n",
      "INFO:tensorflow:global step 144: loss = 5.1151 (26.38 sec/step)\n",
      "INFO:tensorflow:global step 146: loss = 5.1246 (22.92 sec/step)\n",
      "INFO:tensorflow:global step 148: loss = 5.0775 (22.37 sec/step)\n",
      "INFO:tensorflow:global step 150: loss = 5.6718 (22.59 sec/step)\n",
      "INFO:tensorflow:global step 152: loss = 4.9508 (23.97 sec/step)\n",
      "INFO:tensorflow:global step 154: loss = 4.9779 (21.87 sec/step)\n",
      "INFO:tensorflow:global step 156: loss = 4.7793 (23.39 sec/step)\n",
      "INFO:tensorflow:global step 158: loss = 5.1167 (22.71 sec/step)\n",
      "INFO:tensorflow:global step 160: loss = 5.4087 (22.38 sec/step)\n",
      "INFO:tensorflow:global step 162: loss = 5.0835 (23.21 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0417233\n",
      "INFO:tensorflow:global step 164: loss = 5.5506 (47.29 sec/step)\n",
      "INFO:tensorflow:global step 166: loss = 4.8689 (22.62 sec/step)\n",
      "INFO:tensorflow:global step 168: loss = 5.4940 (22.69 sec/step)\n",
      "INFO:tensorflow:global step 170: loss = 4.9424 (22.02 sec/step)\n",
      "INFO:tensorflow:global step 172: loss = 4.7048 (26.24 sec/step)\n",
      "INFO:tensorflow:global step 174: loss = 5.5522 (26.69 sec/step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e96278cac718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputImage/train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-814cada4b37e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_dir, train_inception)\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0mnumber_of_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0minit_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m           saver=saver)\n\u001b[0m",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_op, logdir, train_step_fn, train_step_kwargs, log_every_n_steps, graph, master, is_chief, global_step, number_of_steps, init_op, init_feed_dict, local_init_op, init_fn, ready_op, summary_op, save_summaries_secs, summary_writer, startup_delay_steps, saver, save_interval_secs, sync_optimizer, session_config, trace_every_n_steps)\u001b[0m\n\u001b[1;32m    780\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             total_loss, should_stop = train_step_fn(\n\u001b[0;32m--> 782\u001b[0;31m                 sess, train_op, global_step, train_step_kwargs)\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m               \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stopping Training.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.pyc\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(sess, train_op, global_step, train_step_kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m   total_loss, np_global_step = sess.run([train_op, global_step],\n\u001b[1;32m    529\u001b[0m                                         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace_run_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                                         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    531\u001b[0m   \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juandavid/anaconda/envs/tensorflow12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(\"outputImage/train\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-17bf72245a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow12]",
   "language": "python",
   "name": "conda-env-tensorflow12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
